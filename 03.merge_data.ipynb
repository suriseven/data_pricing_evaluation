{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import csv\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from glob import glob\n",
    "\n",
    "def extract_data_from_sheet(sheet):\n",
    "    data_name = None\n",
    "    data_description = None\n",
    "    data_price = None\n",
    "    schema_cols = []\n",
    "\n",
    "    data_product_cells = []\n",
    "    data_basic_price_cell = None\n",
    "    table_name_cell = None\n",
    "\n",
    "    for row in sheet.iter_rows(values_only=False):\n",
    "        for cell in row:\n",
    "            if cell.value == \"데이터 상품 정보\":\n",
    "                data_product_cells.append(cell)\n",
    "            elif cell.value == \"데이터 기본 이용료\":\n",
    "                data_basic_price_cell = cell\n",
    "            elif cell.value == \"테이블명(한글)\":\n",
    "                table_name_cell = cell\n",
    "\n",
    "    if len(data_product_cells) == 2:\n",
    "        right_1 = sheet.cell(row=data_product_cells[0].row, column=data_product_cells[0].column + 1).value\n",
    "        right_2 = sheet.cell(row=data_product_cells[1].row, column=data_product_cells[1].column + 1).value\n",
    "        data_name = str(right_1).strip() if right_1 else None\n",
    "        data_description = str(right_2).strip().replace(\"\\n\", \"\\\\n\") if right_2 else None\n",
    "\n",
    "    if data_basic_price_cell:\n",
    "        price_cell = sheet.cell(row=data_basic_price_cell.row, column=data_basic_price_cell.column + 1)\n",
    "        data_price = str(price_cell.value).strip() if price_cell.value else None\n",
    "\n",
    "    if table_name_cell:\n",
    "        row_idx = table_name_cell.row + 1\n",
    "        col_idx = table_name_cell.column\n",
    "        while True:\n",
    "            cell = sheet.cell(row=row_idx, column=col_idx)\n",
    "            if cell.value is None or str(cell.value).strip() == \"\":\n",
    "                break\n",
    "            schema_cols.append(str(cell.value).strip())\n",
    "            row_idx += 1\n",
    "\n",
    "    return data_name, data_description, data_price, schema_cols\n",
    "\n",
    "\n",
    "def process_single_file(excel_path, output_dir):\n",
    "    wb = openpyxl.load_workbook(excel_path, data_only=True)\n",
    "    filename = os.path.basename(excel_path)\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_extracted.csv\")\n",
    "\n",
    "    with open(output_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow([\"filename\", \"sheet_name\", \"data_name\", \"data_description\", \"data_price\", \"schema\", \"valid\"])\n",
    "\n",
    "        for sheet_name in wb.sheetnames:\n",
    "            sheet = wb[sheet_name]\n",
    "            name, desc, price, schema = extract_data_from_sheet(sheet)\n",
    "            schema_str = \"|\".join(schema).replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "            is_valid = all([\n",
    "                name and name.strip(),\n",
    "                desc and desc.strip(),\n",
    "                price and price.strip(),\n",
    "                len(schema) > 0\n",
    "            ])\n",
    "            valid_str = \"True\" if is_valid else \"False\"\n",
    "\n",
    "            writer.writerow([filename, sheet_name, name, desc, price, schema_str, valid_str])\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def process_all_files_parallel(input_dir, output_dir, max_workers=4):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    excel_files = glob(os.path.join(input_dir, \"*.xlsx\"))\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for excel_path in excel_files:\n",
    "            futures.append(executor.submit(process_single_file, excel_path, output_dir))\n",
    "\n",
    "        for future in futures:\n",
    "            output_file = future.result()\n",
    "            print(f\"Finished processing: {output_file}\")\n",
    "\n",
    "\n",
    "input_directory = \"/home/ubuntu/data_value/bigdata-transportation\"\n",
    "output_directory = \"parallel_output_csvs\"\n",
    "process_all_files_parallel(input_directory, output_directory, max_workers=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683acebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_csv_files(input_dir, output_file):\n",
    "    csv_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "    df_list = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file, encoding=\"utf-8-sig\")\n",
    "        df_list.append(df)\n",
    "\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    merged_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "merge_csv_files(\"parallel_output_csvs\", \"final_merged_output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def parse_html_file_to_csv(html_path, output_dir):\n",
    "    try:\n",
    "        with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "        name = soup.find(\"h2\", class_=\"detail-title\").get_text(strip=True).replace(\"︎\", \"\")\n",
    "\n",
    "        description_tag = soup.find(\"th\", string=\"상품 요약\")\n",
    "        description = description_tag.find_next(\"td\").get_text(strip=True) if description_tag else \"\"\n",
    "\n",
    "        price_tags = soup.find_all(\"p\", class_=\"price\")\n",
    "        if len(price_tags) > 1:\n",
    "            price_text = price_tags[1].get_text(strip=True)\n",
    "        else:\n",
    "            price_text = price_tags[0].get_text(strip=True) if price_tags else None\n",
    "\n",
    "        schema_tag = soup.find(\"th\", string=\"상품 상세설명\")\n",
    "        schema_text = schema_tag.find_next(\"td\").get_text(separator=\"\\n\", strip=True) if schema_tag else \"\"\n",
    "        schema_lines = [line for line in schema_text.split(\"\\n\") if \"*\" in line]\n",
    "        schema = [line.replace(\"\\n\", \"\\\\n\").replace(\"*\", \"\").strip() for line in schema_lines]\n",
    "\n",
    "        data = {\n",
    "            \"filename\": os.path.basename(html_path),\n",
    "            \"sheet_name\": \"\", \n",
    "            \"name\": name,\n",
    "            \"description\": description.replace(\"\\n\", \"\\\\n\"),\n",
    "            \"price\": price_text,\n",
    "            \"schema\": \"; \".join(schema).replace(\"\\n\", \"\\\\n\"),\n",
    "            \"valid\": True\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {html_path}: {e}\")\n",
    "        data = {\n",
    "            \"filename\": os.path.basename(html_path),\n",
    "            \"sheet_name\": \"\",\n",
    "            \"name\": \"\",\n",
    "            \"description\": \"\",\n",
    "            \"price\": \"\",\n",
    "            \"schema\": \"\",\n",
    "            \"valid\": False\n",
    "        }\n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(html_path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{base_filename}_parsed.csv\")\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def find_html_files(root_dir):\n",
    "    html_files = []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for fname in filenames:\n",
    "            if fname.lower().endswith(\".html\"):\n",
    "                html_files.append(os.path.join(dirpath, fname))\n",
    "    return html_files\n",
    "\n",
    "def parse_html_files_parallel_to_separate_csv(root_dir, output_dir, max_workers=4):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    html_files = find_html_files(root_dir)\n",
    "    print(f\"Found {len(html_files)} HTML files.\")\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(parse_html_file_to_csv, path, output_dir): path for path in html_files}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            output_file = future.result()\n",
    "            if output_file:\n",
    "                print(f\"Saved: {output_file}\")\n",
    "            else:\n",
    "                print(f\"Failed: {futures[future]}\")\n",
    "\n",
    "\n",
    "root_directory = \"/home/ubuntu/data_value/KDX\"\n",
    "output_directory = \"separate_parsed_csvs\"\n",
    "parse_html_files_parallel_to_separate_csv(root_directory, output_directory, max_workers=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce235a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_csv_files(input_dir, output_file):\n",
    "    csv_files = glob.glob(os.path.join(input_dir, \"*.csv\"))\n",
    "    df_list = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file, encoding=\"utf-8-sig\")\n",
    "        df_list.append(df)\n",
    "\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    merged_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "merge_csv_files(\"separate_parsed_csvs\", \"final_merged_output_html.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "import traceback\n",
    "\n",
    "\n",
    "df = pd.read_csv('KDX+국가교통데이터오픈마켓+해양수산.csv', dtype=str)\n",
    "\n",
    "with open('price-ids', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    i = line.rstrip()\n",
    "    filename = f'definition_{i}.xlsx'\n",
    "\n",
    "    try:\n",
    "        \n",
    "        url = f\"https://www.bigdata-transportation.kr/frn/prdt/detail?prdtId=PRDTNUM_0000000{i}\"\n",
    "\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)    \n",
    "\n",
    "        time.sleep(3) \n",
    "\n",
    "        price = driver.find_element(By.CLASS_NAME, 'prd-price').text\n",
    "        print(\"Price:\", price)\n",
    "\n",
    "        df.loc[(df['source'] == 'bigdata-transportation') & (df['price'] == '유료') & (df['filename'] == filename) , 'price'] = price\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        df.to_csv('price-KDX+국가교통데이터오픈마켓+해양수산.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(f'Error on file {filename}')\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('price-KDX+국가교통데이터오픈마켓+해양수산.csv', dtype=str)\n",
    "\n",
    "df.loc[(df['source'] == 'bigdata-sea') & (df['price'].isna()), 'price'] = \"0\"\n",
    "df.loc[df['price'] == '무료', 'price'] = \"0\"\n",
    "\n",
    "df['price'] = df['price'].str.replace(',','')\n",
    "df['price'] = df['price'].str.replace(' ','')\n",
    "df['price'] = df['price'].str.replace('원','')\n",
    "\n",
    "df['collect_dt'] = '2025-07-15'\n",
    "\n",
    "df = df[df['valid'] == 'True']\n",
    "\n",
    "df.loc[df['sheet_name'].isna(), 'sheet_name'] = '-'\n",
    "df.loc[df['schema'].isna(), 'schema'] = ''\n",
    "\n",
    "df.to_csv('KDX+국가교통데이터오픈마켓+해양수산_20250725.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df03603",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('KDX+국가교통데이터오픈마켓+해양수산_20250725.csv', dtype=str)\n",
    "\n",
    "df.loc[df['schema'].isna(), 'schema'] = ''\n",
    "\n",
    "df['sorted_schema'] = (\n",
    "    df['schema'].str.split('|')\n",
    "    .apply(sorted)\n",
    "    .apply(lambda x: '-'.join(x))\n",
    ")\n",
    "\n",
    "df.to_csv('KDX+국가교통데이터오픈마켓+해양수산_20250725_sorted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7660005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/ubuntu/data_value/KDX+국가교통데이터오픈마켓+해양수산_20250725_sorted.csv')\n",
    "df['company'] = '-'\n",
    "df['category'] = '-'\n",
    "df['extension'] = '-'\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb161764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "html_dir = '/home/ubuntu/data_value/KDX'\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(html_dir):\n",
    "\n",
    "     \n",
    "    if not filename.endswith('.html'):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(html_dir, filename)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='euc-kr') as f:\n",
    "            html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 초기화\n",
    "    category = None\n",
    "    extension = None\n",
    "    company = None\n",
    "\n",
    "    for row in soup.select('table.table-detail tr'):\n",
    "        th = row.find('th')\n",
    "        td = row.find('td')\n",
    "        if not th or not td:\n",
    "            continue\n",
    "\n",
    "        label = th.get_text(strip=True)\n",
    "        value = td.get_text(strip=True)\n",
    "\n",
    "        if label == '상품 카테고리':\n",
    "            category = value\n",
    "        elif label == '확장자':\n",
    "            extension = value\n",
    "        elif label == '회사명':\n",
    "            company = value\n",
    "\n",
    "    item = {\n",
    "        '파일명': filename,\n",
    "        '카테고리': category,\n",
    "        '확장자': extension,\n",
    "        '회사명': company\n",
    "    }\n",
    "\n",
    "    df.loc[df['filename'] == filename, 'company'] = company\n",
    "    df.loc[df['filename'] == filename, 'category'] = category\n",
    "    df.loc[df['filename'] == filename, 'extension'] = extension\n",
    "\n",
    "    results.append(item)\n",
    "\n",
    "\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29507f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/ubuntu/data_value/KDX+국가교통데이터오픈마켓+해양수산_20250730_1.csv', dtype=str)\n",
    "\n",
    "df['tags'] = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29585a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Directory where HTML files are stored\n",
    "html_dir = '/home/ubuntu/data_value/KDX'\n",
    "all_tags = []\n",
    "\n",
    "# Walk through each HTML file in the directory\n",
    "for filename in os.listdir(html_dir):\n",
    "    if filename.endswith('.html'):\n",
    "        with open(os.path.join(html_dir, filename), 'r', encoding='utf-8') as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            # Find all divs with class 'tag' and attribute 'data-text'\n",
    "            tag_divs = soup.find_all('div', class_='tag', attrs={'data-text': True})\n",
    "            tags = [tag['data-text'] for tag in tag_divs]\n",
    "            all_tags.extend(tags)\n",
    "\n",
    "            print(tags)            \n",
    "            df.loc[df['filename'] == filename, 'tags'] = str(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b195463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/ubuntu/data_value/KDX+국가교통데이터오픈마켓+해양수산_20250730_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79104c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"도로\": {\n",
    "        \"차량\": [\"위치\", \"속도\"]\n",
    "    },\n",
    "    \"도시\": {\n",
    "        \"상권\": [\"학원\", \"카페\", \"편의점\"],\n",
    "        \"환경\": [\"기온\", \"대기\"],\n",
    "        \"인구\": [\"인구세대\"],\n",
    "        \"이동\": [\"유동인구\"],\n",
    "        \"재무\": [\"소득/소비\", \"부채\", \"신용\", \"대출\", \"자산\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"/home/ubuntu/data_value/KDX+국가교통데이터오픈마켓+해양수산_20250730_1.csv\")  # make sure 'description' column exists\n",
    "\n",
    "# Clean and tokenize\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "df['tokens'] = df['description'].apply(lambda x: tokenize(x) if isinstance(x, str) else [])\n",
    "\n",
    "\n",
    "# Flatten all tokens\n",
    "all_tokens = [word for tokens in df['tokens'] for word in tokens]\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "# Top 50 most common words\n",
    "top_words = word_freq.most_common(10000)\n",
    "for word, freq in top_words:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
